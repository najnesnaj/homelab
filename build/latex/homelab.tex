%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Homelab}
\date{May 11, 2025}
\release{1}
\author{Jan Jansen}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{intro}
\label{\detokenize{intro:intro}}\label{\detokenize{intro::doc}}
\sphinxAtStartPar
This doc does not belong here, but did not want to create multiple github repo’s.

\sphinxAtStartPar
This is a personal account of venturing into the Proxmox virutualised world:
\begin{itemize}
\item {} 
\sphinxAtStartPar
running containers

\item {} 
\sphinxAtStartPar
running local LLM

\item {} 
\sphinxAtStartPar
no GPU

\item {} 
\sphinxAtStartPar
cheap

\end{itemize}

\sphinxAtStartPar
I try to document as much as I can, to avoid solving the same problem twice.


\section{homelab}
\label{\detokenize{intro:homelab}}
\sphinxAtStartPar
I bought hp dl380p g8 (generation8) with 16 cores and 32 threads and 256 GIGA bytes of RAM!! for the price of a raspberry pi.

\sphinxAtStartPar
I plan it on using during winter, so its heat is not lost. During rendering or running a LLM it generates 400 Watt. (or consumes for 400 watt expensive electricity)

\sphinxAtStartPar
It could use a GPU, but I hate to spend more money, and it would need a special riser to give way to PCIe x16 double slot.

\sphinxAtStartPar
results sofar :
\begin{itemize}
\item {} 
\sphinxAtStartPar
LLM runs at 5 tokens / second (llavafile)

\item {} 
\sphinxAtStartPar
blender takes half an hour for rendering (CYCLES) a single picture

\item {} 
\sphinxAtStartPar
using it as a  ramdisk give 1,5G/s readspead!

\end{itemize}


\section{software}
\label{\detokenize{intro:software}}
\sphinxAtStartPar
I choose proxmox as a virtualisation platform, it runs linux containers (LXC), which are kind of cool since they are created quickly, launched quickly. Some problems arise since there are still shared resources with the host… hence the use of the included templates

\sphinxstepscope


\chapter{Upgrading homelab HP DL 380p}
\label{\detokenize{hpdl380p:upgrading-homelab-hp-dl-380p}}\label{\detokenize{hpdl380p::doc}}

\section{latest BIOS}
\label{\detokenize{hpdl380p:latest-bios}}\begin{itemize}
\item {} 
\sphinxAtStartPar
download latest firmware (BIOS) for linux in RPM format

\item {} 
\sphinxAtStartPar
firmware\sphinxhyphen{}system\sphinxhyphen{}p70\sphinxhyphen{}2019.05.24\sphinxhyphen{}1.1

\item {} 
\sphinxAtStartPar
on ubuntu (unpack with Ark)

\item {} 
\sphinxAtStartPar
look for CPQP7013.6B8 (4MB in size)

\item {} 
\sphinxAtStartPar
use ilo to upload firmware (update)

\end{itemize}


\section{M2 disk drive}
\label{\detokenize{hpdl380p:m2-disk-drive}}\begin{itemize}
\item {} 
\sphinxAtStartPar
M.2 NGFF SSD naar PCI\sphinxhyphen{}E 3.0 X16 High\sphinxhyphen{}Speed SSD  (ashata = 5euro)

\item {} 
\sphinxAtStartPar
need MVME (one notch) m2 card

\item {} 
\sphinxAtStartPar
it shows up in BIOS / PCI devices

\item {} 
\sphinxAtStartPar
on linux :\#lsblk it should show up

\end{itemize}


\section{Sata}
\label{\detokenize{hpdl380p:sata}}\begin{itemize}
\item {} 
\sphinxAtStartPar
m2 to sata adapter case (aliexpress)

\item {} 
\sphinxAtStartPar
cable female sata to female Slimline 13pin 7 + 6 (aliexpress)

\item {} 
\sphinxAtStartPar
m2 sata disk 128G

\end{itemize}


\section{Bootconfig}
\label{\detokenize{hpdl380p:bootconfig}}
\sphinxAtStartPar
there is a
\sphinxhyphen{} SAS controller
\sphinxhyphen{} SATA controller (cdrom)

\sphinxAtStartPar
the controllor has a bootorder as well !!!!
In order to boot from sata, the sata controller has to boot first!!!

\sphinxAtStartPar
{\color{red}\bfseries{}**}this is the way to boot an expensive hp server from a simple 2,5 inch sata disk (laptop 5V) ,using the onboard slimline cd\sphinxhyphen{}rom connector

\sphinxstepscope


\chapter{AVX}
\label{\detokenize{avx:avx}}\label{\detokenize{avx::doc}}
\sphinxAtStartPar
\sphinxurl{https://github.com/Mintplex-Labs/anything-llm/issues/1331}

\sphinxAtStartPar
I ran into a problem using lancedb container that needs avx2.
turns out that my old hp dl380p g8 with xeon e5\sphinxhyphen{}2650 v2 does not have AVX2….

\sphinxAtStartPar
AVX (Advanced Vector Extensions) and AVX2 are sets of CPU instructions designed to improve performance for tasks that involve heavy mathematical computations, like simulations, scientific calculations, or multimedia processing.
\begin{quote}
\begin{description}
\sphinxlineitem{AVX (Advanced Vector Extensions):}
\sphinxAtStartPar
Introduced with Intel’s Sandy Bridge processors (2011).
Supports 256\sphinxhyphen{}bit wide vector operations, allowing for parallel processing of multiple data points in a single instruction. This improves performance in applications like video processing, scientific simulations, and cryptographic tasks.

\sphinxlineitem{AVX2 (Advanced Vector Extensions 2):}
\sphinxAtStartPar
Introduced with Intel’s Haswell processors (2013).
Expands on AVX by supporting more operations and improving the handling of integer operations. AVX2 also increases the vector size to 256 bits for integer data and introduces FMA (Fused Multiply\sphinxhyphen{}Add), which further boosts performance in floating\sphinxhyphen{}point operations.

\end{description}
\end{quote}

\sphinxAtStartPar
Can the Xeon E5\sphinxhyphen{}2650 v2 Support AVX2?

\sphinxAtStartPar
No, the Intel Xeon E5\sphinxhyphen{}2650 v2 does not support AVX2. This processor, part of the Ivy Bridge\sphinxhyphen{}EP family, supports AVX but not AVX2, as AVX2 was introduced with the later Haswell architecture.
Je zei:
what do they mean : To resolve the issue all I had to do was update the docker run command with the lancedb\_revert tag,

\sphinxstepscope


\chapter{nvme}
\label{\detokenize{nvme:nvme}}\label{\detokenize{nvme::doc}}
\sphinxAtStartPar
insert M2 into pci

\sphinxAtStartPar
check temperature :
nvme smart\sphinxhyphen{}log /dev/nvme0
Smart Log for NVME device:nvme0 namespace\sphinxhyphen{}id:ffffffff
critical\_warning                        : 0
temperature                             : 33°C (306 Kelvin)
available\_spare                         : 100\%
available\_spare\_threshold               : 10\%
percentage\_used                         : 0\%
endurance group critical warning summary: 0
Data Units Read                         : 7,871 (4.03 GB)
Data Units Written                      : 167,622 (85.82 GB)
host\_read\_commands                      : 152,308
host\_write\_commands                     : 676,518
controller\_busy\_time                    : 1
power\_cycles                            : 2
power\_on\_hours                          : 0
unsafe\_shutdowns                        : 2
media\_errors                            : 0
num\_err\_log\_entries                     : 0
Warning Temperature Time                : 0
Critical Composite Temperature Time     : 0
Temperature Sensor 1           : 33°C (306 Kelvin)
Thermal Management T1 Trans Count       : 0
Thermal Management T2 Trans Count       : 0
Thermal Management T1 Total Time        : 0
Thermal Management T2 Total Time        : 0

\sphinxstepscope


\chapter{Where can I find more Templates?}
\label{\detokenize{templates:where-can-i-find-more-templates}}\label{\detokenize{templates::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Use:

\end{itemize}

\sphinxAtStartPar
pveam update
\begin{itemize}
\item {} 
\sphinxAtStartPar
to update the container template database, then:

\end{itemize}

\sphinxAtStartPar
pveam available

\sphinxstepscope


\chapter{reading data from USB stick}
\label{\detokenize{usb:reading-data-from-usb-stick}}\label{\detokenize{usb::doc}}
\sphinxAtStartPar
the USB stick is readable from the proxmox host:

\sphinxAtStartPar
(do a dmesg to get the device: in this case /dev/sdb1)
mount /dev/sdb1 /usbdrive


\section{create a mp on CT (container02)}
\label{\detokenize{usb:create-a-mp-on-ct-container02}}
\sphinxAtStartPar
/dev/mapper/pve\sphinxhyphen{}vm\textendash{}102\textendash{}disk\textendash{}1  51290592        28  48652740   1\% /container02mp


\section{transfer to CT (name = container02)}
\label{\detokenize{usb:transfer-to-ct-name-container02}}
\sphinxAtStartPar
on the host
mkdir /drive\_container02
mount /dev/mapper/pve\sphinxhyphen{}vm\textendash{}102\textendash{}disk\textendash{}1 /drive\_container02/

\sphinxstepscope


\chapter{ramdisk}
\label{\detokenize{ramdisk:ramdisk}}\label{\detokenize{ramdisk::doc}}
\sphinxAtStartPar
My dl380p gen8 has 256Gb of RAM


\section{Can I use RAM as a disk?}
\label{\detokenize{ramdisk:can-i-use-ram-as-a-disk}}
\sphinxAtStartPar
mkdir /tmp/ramdisk
chmod 777 /tmp/ramdisk
mount \sphinxhyphen{}t tmpfs \sphinxhyphen{}o size=1024m myramdisk /tmp/ramdisk

\sphinxAtStartPar
or to get something extra…

\sphinxAtStartPar
sudo mount \sphinxhyphen{}t tmpfs \sphinxhyphen{}o size=10G myramdisk /tmp/ramdisk


\section{speedtest}
\label{\detokenize{ramdisk:speedtest}}
\sphinxAtStartPar
For Write:
dd if=/dev/zero of=/dev/shm/ram bs=1048576 count=4096 oflag=nocache conv=fsync
4096+0 records in
4096+0 records out
4294967296 bytes (4.3 GB, 4.0 GiB) copied, 2.79948 s, 1.5 GB/s

\sphinxAtStartPar
or:
dd if=/dev/zero of=/tmp/ramdisk/blok bs=1048576 count=1024 oflag=nocache conv=fsync
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.560324 s, 1.9 GB/s

\sphinxAtStartPar
For Read:

\sphinxAtStartPar
dd if=/tmp/ramdisk/blok of=/dev/null bs=1048576 iflag=nocache,sync conv=nocreat

\sphinxAtStartPar
dd if=/tmp/ramdisk/blok of=/dev/null bs=1048576 iflag=nocache,sync conv=nocreat
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.240446 s, 4.5 GB/s


\bigskip\hrule\bigskip


\sphinxAtStartPar
modprobe zram
echo 80G | tee /sys/block/zram0/disksize    (80G ramdisk)
mkfs.ext4 /dev/zram0  (make a filesystem)
mkdir /RAM   (create a mountingpoint)
mount /dev/zram0 /RAM

\sphinxAtStartPar
now you can use the /RAM directory (which will be gone after poweroff)

\sphinxstepscope


\chapter{sharing a directory for LXC}
\label{\detokenize{share:sharing-a-directory-for-lxc}}\label{\detokenize{share::doc}}
\sphinxAtStartPar
on the host (pve node) create a shared directory (jan): (created on M2 storage)
chown \sphinxhyphen{}R nobody:nogroup jan
chmod 777 jan

\sphinxAtStartPar
\sphinxhref{mailto:root@pve}{root@pve}:/etc/pve/lxc\#


\section{modify the lxc}
\label{\detokenize{share:modify-the-lxc}}
\sphinxAtStartPar
create a directory /mnt/shared (which will be the mounting point)


\section{on PVE modify the lxc config}
\label{\detokenize{share:on-pve-modify-the-lxc-config}}
\sphinxstepscope


\chapter{moving a LXC container}
\label{\detokenize{moving:moving-a-lxc-container}}\label{\detokenize{moving::doc}}
\sphinxAtStartPar
I had a container on a logical volume SCSI and I wanted to move it to logical volume M2

\sphinxAtStartPar
Cloning?

\sphinxAtStartPar
The container shared a directory, and cloning and displacing would mess this up.

\sphinxAtStartPar
Solution:
backup \& restore from backup on other volume

\sphinxstepscope


\chapter{nfs network between linux containers}
\label{\detokenize{nfs_netwerk:nfs-network-between-linux-containers}}\label{\detokenize{nfs_netwerk::doc}}

\section{set up a bridge}
\label{\detokenize{nfs_netwerk:set-up-a-bridge}}
\sphinxAtStartPar
A Linux bridge interface (commonly called vmbrX) is needed to connect guests to the underlying physical network. It can be thought of as a virtual switch which the guests and physical interfaces are connected to.


\section{define an extra network interface in range 10.0.0.X}
\label{\detokenize{nfs_netwerk:define-an-extra-network-interface-in-range-10-0-0-x}}
\sphinxstepscope


\chapter{NFS Host and Client Setup in Proxmox}
\label{\detokenize{nfs_proxmox_setup:nfs-host-and-client-setup-in-proxmox}}\label{\detokenize{nfs_proxmox_setup::doc}}
\sphinxAtStartPar
This guide will explain how to set up an NFS (Network File Sharing) server and add it as a remote storage in Proxmox.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Install NFS Server}

\sphinxAtStartPar
First, log in to the LXC container or the machine where the NFS server will be hosted. Then update the package list and install NFS:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
apt\PYGZhy{}get\PYG{+w}{ }update
apt\PYGZhy{}get\PYG{+w}{ }install\PYG{+w}{ }sudo\PYG{+w}{ }\PYGZhy{}y
sudo\PYG{+w}{ }apt\PYG{+w}{ }install\PYG{+w}{ }nfs\PYGZhy{}kernel\PYGZhy{}server
\end{sphinxVerbatim}

\sphinxAtStartPar
Once installed, create a shared folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo\PYG{+w}{ }mkdir\PYG{+w}{ }/home/sharedfolder
sudo\PYG{+w}{ }chmod\PYG{+w}{ }\PYG{l+m}{777}\PYG{+w}{ }/home/sharedfolder
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, edit the \sphinxcode{\sphinxupquote{/etc/exports}} file to configure the shared directory for export:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo\PYG{+w}{ }nano\PYG{+w}{ }/etc/exports
\end{sphinxVerbatim}

\sphinxAtStartPar
Add the following line (adjust the IP address and folder accordingly):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/home/sharedfolder\PYG{+w}{ }\PYG{l+m}{192}.168.1.0/24\PYG{o}{(}rw,sync,no\PYGZus{}subtree\PYGZus{}check\PYG{o}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Save the file and restart the NFS server:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo\PYG{+w}{ }exportfs\PYG{+w}{ }\PYGZhy{}ra
sudo\PYG{+w}{ }systemctl\PYG{+w}{ }restart\PYG{+w}{ }nfs\PYGZhy{}kernel\PYGZhy{}server
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Configure NFS Storage in Proxmox}

\sphinxAtStartPar
Now log into your Proxmox host (the machine that will receive the NFS storage) and navigate to:

\sphinxAtStartPar
\sphinxstyleemphasis{Datacenter \textgreater{} Storage \textgreater{} Add \textgreater{} NFS}

\noindent\sphinxincludegraphics{{firefox_7FrqsKGmm4}.png}

\sphinxAtStartPar
Here, enter the NFS server’s IP address and select the shared directory.

\noindent\sphinxincludegraphics{{firefox_85gotgwjhw}.png}

\sphinxAtStartPar
Choose the desired contents for the storage (ISO images, containers, backups, etc.) and click \sphinxstyleemphasis{Add}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Set Permissions on LXC Container (If Applicable)}

\sphinxAtStartPar
If the NFS share will be used in an LXC container, ensure that permissions for NFS usage are set correctly:

\noindent\sphinxincludegraphics{{firefox_1mRMoEMAtl}.png}

\sphinxAtStartPar
Check the \sphinxstyleemphasis{NFS} box under \sphinxstyleemphasis{Options} for the LXC container.

\end{enumerate}

\sphinxAtStartPar
That’s it! You have successfully set up an NFS server and added it to Proxmox as remote storage.

\sphinxstepscope


\chapter{nfs client}
\label{\detokenize{nfs_client:nfs-client}}\label{\detokenize{nfs_client::doc}}
\sphinxAtStartPar
apt install nfs\sphinxhyphen{}common

\sphinxAtStartPar
mount \sphinxhyphen{}t nfs 10.0.0.104:/share /mnt/nfson104


\section{under node pve}
\label{\detokenize{nfs_client:under-node-pve}}
\sphinxAtStartPar
watch out for features

\sphinxstepscope


\chapter{Portainer}
\label{\detokenize{portainer:portainer}}\label{\detokenize{portainer::doc}}

\section{using a script}
\label{\detokenize{portainer:using-a-script}}
\sphinxAtStartPar
use a template for the linux container

\sphinxAtStartPar
a script from :
\sphinxurl{https://raw.githubusercontent.com/tteck/Proxmox/refs/heads/main/install/docker-install.sh}

\sphinxAtStartPar
systemctl start docker
systemctl status docker

\sphinxAtStartPar
\sphinxurl{https://192.168.0.182:9443} (your IP)


\section{by hand}
\label{\detokenize{portainer:by-hand}}\begin{itemize}
\item {} 
\sphinxAtStartPar
create CT ubuntu22 with template

\item {} 
\sphinxAtStartPar
apt update

\item {} 
\sphinxAtStartPar
sudo apt install docker.io \sphinxhyphen{}y

\item {} 
\sphinxAtStartPar
sudo systemctl status docker

\item {} 
\sphinxAtStartPar
sudo usermod \sphinxhyphen{}aG docker \$USER (add current logged on user to docker group)

\item {} 
\sphinxAtStartPar
docker pull portainer/portainer\sphinxhyphen{}ce:latest

\item {} 
\sphinxAtStartPar
docker run \sphinxhyphen{}d \sphinxhyphen{}p 9000:9000 \textendash{}restart always \sphinxhyphen{}v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\sphinxhyphen{}ce:latest

\end{itemize}


\section{exporting \& importing}
\label{\detokenize{portainer:exporting-importing}}
\sphinxAtStartPar
this seems to work between systems:

\sphinxAtStartPar
(origin) sudo docker save ollama/ollama:latest  \textgreater{} my\sphinxhyphen{}ollama.tar
(target) sudo docker load \textless{} my\sphinxhyphen{}ollama.tar

\sphinxstepscope


\chapter{server for blender}
\label{\detokenize{blender:server-for-blender}}\label{\detokenize{blender::doc}}
\sphinxAtStartPar
Although the hp dl380 was 5 times faster than my i7 laptop, it is still slow, takes half an hour to render a picture.

\sphinxAtStartPar
One can adjust setting within blender to speed up things a bit, but still …


\section{howto render?}
\label{\detokenize{blender:howto-render}}\begin{itemize}
\item {} 
\sphinxAtStartPar
download blender 4.2

\end{itemize}

\sphinxAtStartPar
cd blender\sphinxhyphen{}4.2.0\sphinxhyphen{}linux\sphinxhyphen{}x64/

\sphinxAtStartPar
./blender \sphinxhyphen{}b /home/naj/misvormde\sphinxhyphen{}donut1.blend \sphinxhyphen{}E CYCLES \sphinxhyphen{}f 1


\section{faster / less good}
\label{\detokenize{blender:faster-less-good}}
\sphinxAtStartPar
./blender \sphinxhyphen{}b /home/naj/misvormde\sphinxhyphen{}donut1.blend \sphinxhyphen{}E BLENDER\_EEVEE\_NEXT \sphinxhyphen{}f 1


\section{at the movies}
\label{\detokenize{blender:at-the-movies}}
\sphinxAtStartPar
./blender \sphinxhyphen{}b /home/naj/misvormde\sphinxhyphen{}donut1.blend \sphinxhyphen{}E BLENDER\_EEVEE\_NEXT \sphinxhyphen{}s 10 \sphinxhyphen{}e 500 \sphinxhyphen{}t 2 \sphinxhyphen{}a
./blender \sphinxhyphen{}b /home/naj/misvormde\sphinxhyphen{}donut1.blend \sphinxhyphen{}E BLENDER\_EEVEE\_NEXT \sphinxhyphen{}s 1 \sphinxhyphen{}e 100 \sphinxhyphen{}t 2 \sphinxhyphen{}a

\sphinxstepscope


\chapter{ollama install and use}
\label{\detokenize{ollama:ollama-install-and-use}}\label{\detokenize{ollama::doc}}
\sphinxAtStartPar
curl \sphinxhyphen{}fsSL \sphinxurl{https://ollama.com/install.sh} | sh


\section{using a container}
\label{\detokenize{ollama:using-a-container}}
\sphinxAtStartPar
ollama\sphinxhyphen{}model\sphinxhyphen{}gemma2 was mounted using a volume and the image exported

\sphinxAtStartPar
sudo docker import ollama.tar ollama:latest

\sphinxstepscope


\chapter{docker}
\label{\detokenize{docker:docker}}\label{\detokenize{docker::doc}}
\sphinxAtStartPar
using ubuntu 22, I found out that the docker that came with it does not work like it should
\begin{itemize}
\item {} 
\sphinxAtStartPar
uninstall docker

\end{itemize}

\sphinxAtStartPar
docker version
Client: Docker Engine \sphinxhyphen{} Community
\begin{quote}

\sphinxAtStartPar
Version:           27.3.1
API version:       1.47
Go version:        go1.22.7
Git commit:        ce12230
Built:             Fri Sep 20 11:41:00 2024
OS/Arch:           linux/amd64
\end{quote}


\chapter{running openwebui}
\label{\detokenize{docker:running-openwebui}}\begin{itemize}
\item {} 
\sphinxAtStartPar
running it from within portainer did not allow to change the host

\item {} 
\sphinxAtStartPar
command prompt

\end{itemize}

\sphinxAtStartPar
sudo docker run \sphinxhyphen{}d \sphinxhyphen{}p 3000:8080 \textendash{}add\sphinxhyphen{}host=host.docker.internal:host\sphinxhyphen{}gateway \sphinxhyphen{}v open\sphinxhyphen{}webui:/app/backend/data \textendash{}name open\sphinxhyphen{}webui \textendash{}restart always ghcr.io/open\sphinxhyphen{}webui/open\sphinxhyphen{}webui:main


\chapter{copy data to container}
\label{\detokenize{docker:copy-data-to-container}}
\sphinxAtStartPar
docker cp manifests/ ollama:/root/.ollama/models
Successfully copied 12.8kB to ollama:/root/.ollama/models

\sphinxstepscope


\chapter{configure elasticview}
\label{\detokenize{elastic:configure-elasticview}}\label{\detokenize{elastic::doc}}

\section{elasticsearch password}
\label{\detokenize{elastic:elasticsearch-password}}
\sphinxAtStartPar
connect with term to container:
bin/elasticsearch\sphinxhyphen{}reset\sphinxhyphen{}password \sphinxhyphen{}u elastic
bin/elasticsearch\sphinxhyphen{}reset\sphinxhyphen{}password \sphinxhyphen{}u elastic \sphinxhyphen{}i (this allows for setting it yourself)


\section{testing elasticview connection}
\label{\detokenize{elastic:testing-elasticview-connection}}
\sphinxAtStartPar
curl \sphinxhyphen{}X GET \sphinxhyphen{}k \sphinxhyphen{}u elastic:ebktuBhBHtE7N+JeBbIV “https://192.168.0.121:9200/\_cluster/health/?pretty”
\begin{description}
\sphinxlineitem{\{}
\sphinxAtStartPar
“cluster\_name” : “docker\sphinxhyphen{}cluster”,
“status” : “green”,
“timed\_out” : false,
“number\_of\_nodes” : 1,
“number\_of\_data\_nodes” : 1,
“active\_primary\_shards” : 1,
“active\_shards” : 1,
“relocating\_shards” : 0,
“initializing\_shards” : 0,
“unassigned\_shards” : 0,
“delayed\_unassigned\_shards” : 0,
“number\_of\_pending\_tasks” : 0,
“number\_of\_in\_flight\_fetch” : 0,
“task\_max\_waiting\_in\_queue\_millis” : 0,
“active\_shards\_percent\_as\_number” : 100.0

\end{description}

\sphinxAtStartPar
\}


\section{getting website certificate}
\label{\detokenize{elastic:getting-website-certificate}}
\sphinxAtStartPar
connect to \sphinxurl{http://192.168.0.121}

\sphinxAtStartPar
firefox/settings/privicy\&security/certificates to add exception

\sphinxstepscope


\chapter{mounting}
\label{\detokenize{mounting:mounting}}\label{\detokenize{mounting::doc}}
\sphinxAtStartPar
mkdir /SCSIdata for the data on the scsidisk /dev/sdb
mkdir /M2data for the data on the M2 disk /dev/nvme0n1p2

\sphinxAtStartPar
\# mount /dev/sdb /SCSIdata

\sphinxAtStartPar
\# mount /dev/nvme0n1p2 /M2data


\chapter{edit  /etc/fstab}
\label{\detokenize{mounting:edit-etc-fstab}}
\sphinxAtStartPar
/dev/nvme0n1p2  /M2data  ext4  defaults  0  2
/dev/sdb  /SCSIdata  ext4  defaults  0  2

\sphinxstepscope


\chapter{Backup pve}
\label{\detokenize{backuppve:backup-pve}}\label{\detokenize{backuppve::doc}}
\sphinxAtStartPar
Proxmox node now start from a sata m2 in the CDrom slot

\sphinxAtStartPar
A tar copy is made to M2data from /etc directory

\sphinxAtStartPar
In case of crash : reinstall proxmox on sata disk en copy backup.tar to /etc

\sphinxstepscope


\chapter{RAID}
\label{\detokenize{raid:raid}}\label{\detokenize{raid::doc}}
\sphinxAtStartPar
previously 2 600GB SAS disks were in RAID1 in the same volumegroup.

\sphinxAtStartPar
I have no spare disk, nor do I want to spend money on old tech.

\sphinxAtStartPar
Reconfigure : each disk is within own volume group, no more RAID, more (free) space.
Tool would not let me otherwise.


\section{No more raid :}
\label{\detokenize{raid:no-more-raid}}
\sphinxAtStartPar
machine on one disk are backupped to other disk, and vice versa
one disk remains VG (volume group) within proxmox : used for LXC en VM images, backup
other disk contains ext4 filesystem and is a directory

\sphinxstepscope


\chapter{melborp server}
\label{\detokenize{melborp:melborp-server}}\label{\detokenize{melborp::doc}}

\section{solution for problem running pgadmin container and nginx}
\label{\detokenize{melborp:solution-for-problem-running-pgadmin-container-and-nginx}}
\sphinxAtStartPar
Configure Nginx: Create a new configuration file for your domain in /etc/nginx/sites\sphinxhyphen{}available/melborp.solutions:

\sphinxAtStartPar
nginx
\begin{description}
\sphinxlineitem{server \{}
\sphinxAtStartPar
listen 80;
server\_name www.melborp.solutions;
\begin{description}
\sphinxlineitem{location / \{}
\sphinxAtStartPar
proxy\_pass \sphinxurl{http://localhost:8888};  \# Forward traffic to the pgAdmin container
proxy\_set\_header Host \$host;
proxy\_set\_header X\sphinxhyphen{}Real\sphinxhyphen{}IP \$remote\_addr;
proxy\_set\_header X\sphinxhyphen{}Forwarded\sphinxhyphen{}For \$proxy\_add\_x\_forwarded\_for;
proxy\_set\_header X\sphinxhyphen{}Forwarded\sphinxhyphen{}Proto \$scheme;

\end{description}

\sphinxAtStartPar
\}

\end{description}

\sphinxAtStartPar
\}

\sphinxstepscope


\chapter{Setting up NVIDIA Tesla P100}
\label{\detokenize{NVIDIA:setting-up-nvidia-tesla-p100}}\label{\detokenize{NVIDIA::doc}}
\sphinxAtStartPar
the P100 was choses for balance between performance and price (second hand ebay)


\section{Aliexpress}
\label{\detokenize{NVIDIA:aliexpress}}\begin{quote}

\sphinxAtStartPar
PCIE 4.0 3.0 16X Riser Kabel 90/180 Graden Mount Video Grafisch
\end{quote}

\sphinxAtStartPar
Gpu 10pin Naar 1x8pin Dual 8(6 + 2)Pin Power Adapter Kabel Voor Hp Dl380 Gen8 Gen9 Server


\section{Cutting the Riser}
\label{\detokenize{NVIDIA:cutting-the-riser}}
\sphinxAtStartPar
a special Riser for the HP Proliant gen8 is a good idea because it has a PCIE 16x in the middle which allows for the GPU  to be plugged in. (hard to find and expensive)

\sphinxAtStartPar
I choose the cheap option (evidently) and cut a hole in a riser to extend with a riser cable into this riser. Some more cutting had to be done because the GPU is too long.

\sphinxAtStartPar
For the gen9 there a cheaper riser card options, so no cutting needed…


\section{Configure the BIOS}
\label{\detokenize{NVIDIA:configure-the-bios}}
\sphinxAtStartPar
(changed IRQ for network card to 11, since conflict)

\sphinxAtStartPar
enter BIOS (F9) and on main bios screen / “Service options” menu item. Under this, enable “PCI Express 64\sphinxhyphen{}bit BAR


\section{configure proxmox host}
\label{\detokenize{NVIDIA:configure-proxmox-host}}
\sphinxAtStartPar
PROXMOX PCI\sphinxhyphen{}E GPU passthru

\sphinxAtStartPar
When running proxmox on this hardware, there is more config needed to enable passthru of this GPU to VM.
Enable IOMMU


\section{the idea}
\label{\detokenize{NVIDIA:the-idea}}
\sphinxAtStartPar
the idea is to get nvidia to work on the proxmox host (there will be kernel modules)

\sphinxAtStartPar
for the LXC machines there is no need for kernel drivers!! since already on the host


\section{Download NVIDIA drivers}
\label{\detokenize{NVIDIA:download-nvidia-drivers}}

\section{adapt lxc.conf}
\label{\detokenize{NVIDIA:adapt-lxc-conf}}
\sphinxAtStartPar
/etc/pve/lxc/105.conf

\sphinxAtStartPar
Append these values with the IDs you noted earlier to your file like so. Note the placement of the 195, 234 and 509. This is for a SINGLE gpu also, if you have multiple add additional


\section{copy NVIDIA driver to LXC}
\label{\detokenize{NVIDIA:copy-nvidia-driver-to-lxc}}

\section{Install the NVIDIA Container Toolkit}
\label{\detokenize{NVIDIA:install-the-nvidia-container-toolkit}}

\section{install ollama docker container for GPU usage}
\label{\detokenize{NVIDIA:install-ollama-docker-container-for-gpu-usage}}
\sphinxAtStartPar
\#in portainer you still need to start ollama this way:

\sphinxstepscope


\chapter{Laptop (or PC) mods}
\label{\detokenize{laptop:laptop-or-pc-mods}}\label{\detokenize{laptop::doc}}
\sphinxAtStartPar
/etc/hosts

\sphinxAtStartPar
10.10.10.50 nginx.example.com
10.10.10.50 registry.example.com
\begin{quote}

\sphinxAtStartPar
ip route add 10.10.10.0/24 via 192.168.0.251
\end{quote}

\sphinxAtStartPar
sudo cp ca.crt /usr/local/share/ca\sphinxhyphen{}certificates/ca.crt
sudo update\sphinxhyphen{}ca\sphinxhyphen{}certificates
Updating certificates in /etc/ssl/certs…


\section{make route permanent}
\label{\detokenize{laptop:make-route-permanent}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+lScalar+lScalarPlain}{sudo}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{nano}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{/etc/netplan/01\PYGZhy{}netcfg.yaml}

\PYG{l+lScalar+lScalarPlain}{network}\PYG{p+pIndicator}{:}
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{2}
\PYG{n+nt}{ethernets}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{wlp0s20f3}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{addresses}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.103/24}
\PYG{+w}{    }\PYG{n+nt}{gateway4}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Replace with your actual default gateway}
\PYG{+w}{    }\PYG{n+nt}{routes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{10.10.10.0/24}
\PYG{+w}{        }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.251}

\PYG{+w}{  }\PYG{l+lScalar+lScalarPlain}{sudo}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{netplan}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{apply}
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Proxmox}
\label{\detokenize{cluster:proxmox}}\label{\detokenize{cluster::doc}}
\sphinxAtStartPar
remove firewall from talos worker node

\begin{sphinxVerbatim}[commandchars=\\\{\}]
root@pve:\PYGZti{}\PYGZsh{}\PYG{+w}{ }qm\PYG{+w}{ }\PYG{n+nb}{set}\PYG{+w}{ }\PYG{l+m}{109}\PYG{+w}{ }\PYGZhy{}net0\PYG{+w}{ }virtio,bridge\PYG{o}{=}vmbr0,firewall\PYG{o}{=}\PYG{l+m}{0}
update\PYG{+w}{ }VM\PYG{+w}{ }\PYG{l+m}{109}:\PYG{+w}{ }\PYGZhy{}net0\PYG{+w}{ }virtio,bridge\PYG{o}{=}vmbr0,firewall\PYG{o}{=}\PYG{l+m}{0}
\end{sphinxVerbatim}


\section{setting up an extra network bridge vmbr1}
\label{\detokenize{cluster:setting-up-an-extra-network-bridge-vmbr1}}
\sphinxAtStartPar
on lxc dedicated machine setup dhcp and routing

\begin{sphinxVerbatim}[commandchars=\\\{\}]
apt\PYG{+w}{ }install\PYG{+w}{ }dnsmasq\PYG{+w}{ }\PYGZhy{}y
apt\PYG{+w}{ }install\PYG{+w}{ }iptables\PYGZhy{}persistent\PYG{+w}{ }\PYGZhy{}y

vi\PYG{+w}{ }/etc/dnsmasq.conf
\PYG{n+nv}{interface}\PYG{o}{=}eth0
dhcp\PYGZhy{}range\PYG{o}{=}\PYG{l+m}{10}.10.10.100,10.10.10.200,12h\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} DHCP range for Talos nodes}
dhcp\PYGZhy{}option\PYG{o}{=}\PYG{l+m}{3},10.10.10.2\PYG{+w}{                  }\PYG{c+c1}{\PYGZsh{} Gateway (this machine’s eth0 IP)}
dhcp\PYGZhy{}option\PYG{o}{=}\PYG{l+m}{6},192.168.0.1\PYG{+w}{                 }\PYG{c+c1}{\PYGZsh{} DNS (your home router’s DNS)}

systemctl\PYG{+w}{ }restart\PYG{+w}{ }dnsmasq

\PYG{n+nb}{echo}\PYG{+w}{ }\PYG{l+m}{1}\PYG{+w}{ }\PYGZgt{}\PYG{+w}{ }/proc/sys/net/ipv4/ip\PYGZus{}forward

iptables\PYG{+w}{ }\PYGZhy{}t\PYG{+w}{ }nat\PYG{+w}{ }\PYGZhy{}L\PYG{+w}{ }\PYGZhy{}v
ip\PYG{+w}{ }route\PYG{+w}{ }del\PYG{+w}{ }default\PYG{+w}{ }via\PYG{+w}{ }\PYG{l+m}{10}.10.10.1\PYG{+w}{ }dev\PYG{+w}{ }eth0
ip\PYG{+w}{ }route\PYG{+w}{ }replace\PYG{+w}{ }default\PYG{+w}{ }via\PYG{+w}{ }\PYG{l+m}{192}.168.0.1\PYG{+w}{ }dev\PYG{+w}{ }eth1\PYG{+w}{ }metric\PYG{+w}{ }\PYG{l+m}{100}

iptables\PYG{+w}{ }\PYGZhy{}t\PYG{+w}{ }nat\PYG{+w}{ }\PYGZhy{}A\PYG{+w}{ }POSTROUTING\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }eth1\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }MASQUERADE
ip\PYG{+w}{ }route\PYG{+w}{ }del\PYG{+w}{ }default\PYG{+w}{ }via\PYG{+w}{ }\PYG{l+m}{10}.10.10.1\PYG{+w}{ }dev\PYG{+w}{ }eth0
ip\PYG{+w}{ }route\PYG{+w}{ }replace\PYG{+w}{ }default\PYG{+w}{ }via\PYG{+w}{ }\PYG{l+m}{192}.168.0.1\PYG{+w}{ }dev\PYG{+w}{ }eth1\PYG{+w}{ }metric\PYG{+w}{ }\PYG{l+m}{100}



vi\PYG{+w}{ }/etc/netplan/01\PYGZhy{}netcfg.yaml
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{network}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{2}
\PYG{+w}{  }\PYG{n+nt}{ethernets}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{eth0}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{dhcp4}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\PYG{+w}{      }\PYG{c+c1}{\PYGZsh{} Prevent DHCP from setting a default gateway if it conflicts}
\PYG{+w}{      }\PYG{n+nt}{dhcp4\PYGZhy{}overrides}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{use\PYGZhy{}routes}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\PYG{+w}{        }\PYG{n+nt}{use\PYGZhy{}dns}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\PYG{+w}{        }\PYG{n+nt}{route\PYGZhy{}metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{2000}\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} High metric to prioritize eth1\PYGZsq{}s default route}
\PYG{+w}{    }\PYG{n+nt}{eth1}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{dhcp4}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{false}
\PYG{+w}{      }\PYG{n+nt}{addresses}\PYG{p}{:}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.x/24}\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Replace with your server\PYGZsq{}s IP on this subnet}
\PYG{+w}{      }\PYG{n+nt}{routes}\PYG{p}{:}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{0.0.0.0/0}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{100}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{0.0.0.0/0}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{1024}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.0/24}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{0.0.0.0}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{1024}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{0.0.0.0}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{1024}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{\PYGZlt{}gent.dnscache01\PYGZhy{}ip\PYGZgt{}}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{1024}
\PYG{+w}{        }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{to}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{\PYGZlt{}gent.dnscache02\PYGZhy{}ip\PYGZgt{}}
\PYG{+w}{          }\PYG{n+nt}{via}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{192.168.0.1}
\PYG{+w}{          }\PYG{n+nt}{metric}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{1024}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Apply the netplan configuration}
sudo\PYG{+w}{ }netplan\PYG{+w}{ }generate
sudo\PYG{+w}{ }netplan\PYG{+w}{ }apply

\PYG{c+c1}{\PYGZsh{} Check the routing table}
ip\PYG{+w}{ }route\PYG{+w}{ }show

\PYG{c+c1}{\PYGZsh{} Check iptables rules}
iptables\PYG{+w}{ }\PYGZhy{}t\PYG{+w}{ }nat\PYG{+w}{ }\PYGZhy{}L\PYG{+w}{ }\PYGZhy{}v

\PYG{c+c1}{\PYGZsh{} Check dnsmasq status}
systemctl\PYG{+w}{ }status\PYG{+w}{ }dnsmasq

\PYG{c+c1}{\PYGZsh{} Check if the DHCP server is running and listening on the correct interface}
sudo\PYG{+w}{ }systemctl\PYG{+w}{ }status\PYG{+w}{ }dnsmasq

\PYG{c+c1}{\PYGZsh{} Restart dnsmasq to apply changes}
sudo\PYG{+w}{ }systemctl\PYG{+w}{ }restart\PYG{+w}{ }dnsmasq


netplan\PYG{+w}{ }apply
\end{sphinxVerbatim}


\section{Kernel IP routing table}
\label{\detokenize{cluster:kernel-ip-routing-table}}

\section{using the nodeport}
\label{\detokenize{cluster:using-the-nodeport}}
\sphinxAtStartPar
192.168.0.251:30743

\sphinxAtStartPar
on my router/dhcp on 10.10.10.2 route port to cluster node IP

\sphinxAtStartPar
iptables \sphinxhyphen{}t nat \sphinxhyphen{}A PREROUTING \sphinxhyphen{}i eth1 \sphinxhyphen{}p tcp \textendash{}dport 30743 \sphinxhyphen{}j DNAT \textendash{}to\sphinxhyphen{}destination 10.10.10.118:30743

\sphinxAtStartPar
so running nginx on kubernetes on 10.10.10.255 network is accessible from the outside


\section{using the IP address}
\label{\detokenize{cluster:using-the-ip-address}}
\sphinxAtStartPar
traefik      LoadBalancer   10.102.122.212   10.10.10.50   80:32178/TCP,443:32318/TCP   75m   app.kubernetes.io/instance=traefik\sphinxhyphen{}default,app.kubernetes.io/name=traefik

\sphinxAtStartPar
So now I have to figure out how I can reach  10.10.10.50 from my 192.168.X.X network

\sphinxAtStartPar
on the kubernetes cluster, traefik has been deployed as well as metallb.
iptables \sphinxhyphen{}t nat \sphinxhyphen{}A POSTROUTING \sphinxhyphen{}s 192.168.0.0/24 \sphinxhyphen{}d 10.10.10.0/24 \sphinxhyphen{}j MASQUERADE
sh \sphinxhyphen{}c “iptables\sphinxhyphen{}save \textgreater{} /etc/iptables/rules.v4”

\sphinxAtStartPar
this has been added to th dnsmasq.conf

\sphinxAtStartPar
\# Listen on the 192.168.0.251 interface
interface=eth1  \# Replace with your 192.168.0.251 interface (check with \sphinxtitleref{ip a})
listen\sphinxhyphen{}address=192.168.0.251

\sphinxAtStartPar
\# Forward other queries to upstream DNS (e.g., Google DNS)
server=8.8.8.8
server=8.8.4.4

\sphinxAtStartPar
\# Optional: If LXC is your DHCP server, ensure DNS is offered
dhcp\sphinxhyphen{}option=6,192.168.0.251  \# Tells DHCP clients to use this as DNS


\section{modify dns config on laptop}
\label{\detokenize{cluster:modify-dns-config-on-laptop}}
\sphinxAtStartPar
/etc/resolv.conf

\sphinxAtStartPar
add : nameserver 192.168.0.251


\section{access http://nginx.example.com/ on talos within 10.10.10.X from 192.168.X.X}
\label{\detokenize{cluster:access-http-nginx-example-com-on-talos-within-10-10-10-x-from-192-168-x-x}}
\sphinxAtStartPar
(configure metallb, traefik, nginx)

\sphinxAtStartPar
on laptop /etc/hosts : 10.10.10.50 nginx.example.com

\sphinxAtStartPar
on dhcp server (10.10.10.2)

\sphinxAtStartPar
iptables \sphinxhyphen{}A FORWARD \sphinxhyphen{}s 192.168.0.0/24 \sphinxhyphen{}d 10.10.10.0/24 \sphinxhyphen{}j ACCEPT
iptables \sphinxhyphen{}A FORWARD \sphinxhyphen{}s 10.10.10.0/24 \sphinxhyphen{}d 192.168.0.0/24 \sphinxhyphen{}j ACCEPT

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generated by iptables\PYGZhy{}save v1.8.7 on Thu Apr 10 13:32:59 2025}
*filter
:INPUT\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{0}:0\PYG{o}{]}
:FORWARD\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{0}:0\PYG{o}{]}
:OUTPUT\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{0}:0\PYG{o}{]}
\PYGZhy{}A\PYG{+w}{ }FORWARD\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{192}.168.0.0/24\PYG{+w}{ }\PYGZhy{}d\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }ACCEPT
\PYGZhy{}A\PYG{+w}{ }FORWARD\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}d\PYG{+w}{ }\PYG{l+m}{192}.168.0.0/24\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }ACCEPT
COMMIT
\PYG{c+c1}{\PYGZsh{} Completed on Thu Apr 10 13:32:59 2025}
\PYG{c+c1}{\PYGZsh{} Generated by iptables\PYGZhy{}save v1.8.7 on Thu Apr 10 13:32:59 2025}
*nat
:PREROUTING\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{6847}:1975161\PYG{o}{]}
:INPUT\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{158}:15156\PYG{o}{]}
:OUTPUT\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{25}:2590\PYG{o}{]}
:POSTROUTING\PYG{+w}{ }ACCEPT\PYG{+w}{ }\PYG{o}{[}\PYG{l+m}{25}:2590\PYG{o}{]}
\PYGZhy{}A\PYG{+w}{ }PREROUTING\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }eth1\PYG{+w}{ }\PYGZhy{}p\PYG{+w}{ }tcp\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }tcp\PYG{+w}{ }\PYGZhy{}\PYGZhy{}dport\PYG{+w}{ }\PYG{l+m}{30743}\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }DNAT\PYG{+w}{ }\PYGZhy{}\PYGZhy{}to\PYGZhy{}destination\PYG{+w}{ }\PYG{l+m}{10}.10.10.118:30743
\PYGZhy{}A\PYG{+w}{ }POSTROUTING\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }eth1\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }MASQUERADE
\PYGZhy{}A\PYG{+w}{ }POSTROUTING\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }eth1\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }MASQUERADE
\PYGZhy{}A\PYG{+w}{ }POSTROUTING\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }eth1\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }MASQUERADE
\PYGZhy{}A\PYG{+w}{ }POSTROUTING\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }\PYG{l+m}{192}.168.0.0/24\PYG{+w}{ }\PYGZhy{}d\PYG{+w}{ }\PYG{l+m}{10}.10.10.0/24\PYG{+w}{ }\PYGZhy{}j\PYG{+w}{ }MASQUERADE
COMMIT
\PYG{c+c1}{\PYGZsh{} Completed on Thu Apr 10 13:32:59 2025}
\PYG{c+c1}{\PYGZsh{} Check the iptables rules}
iptables\PYG{+w}{ }\PYGZhy{}t\PYG{+w}{ }nat\PYG{+w}{ }\PYGZhy{}L\PYG{+w}{ }\PYGZhy{}v
iptables\PYG{+w}{ }\PYGZhy{}L\PYG{+w}{ }\PYGZhy{}v

\PYG{c+c1}{\PYGZsh{} Check the routing table}
ip\PYG{+w}{ }route\PYG{+w}{ }show

\PYG{c+c1}{\PYGZsh{} Check the network interfaces}
ip\PYG{+w}{ }a
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Talos}
\label{\detokenize{talos:talos}}\label{\detokenize{talos::doc}}
\sphinxAtStartPar
Talos is a modern OS for Kubernetes. It is designed to be secure, immutable, and minimal. Talos is a self\sphinxhyphen{}hosted Kubernetes distribution that runs on bare metal or virtualized infrastructure. Talos is designed to be managed by a central Kubernetes control plane, which can be hosted on the same cluster or on a separate cluster.

\sphinxAtStartPar
talosctl config add my\sphinxhyphen{}cluster \textendash{}endpoints 192.168.0.242

\sphinxAtStartPar
talosctl config info

\sphinxAtStartPar
talosctl config endpoint 192.168.0.242

\sphinxAtStartPar
talosctl gen config my\sphinxhyphen{}cluster \sphinxurl{https://192.168.0.242:6443} \textendash{}output\sphinxhyphen{}dir ./talos\sphinxhyphen{}config \textendash{}force


\section{new install talos}
\label{\detokenize{talos:new-install-talos}}
\sphinxAtStartPar
\sphinxurl{https://www.talos.dev/v1.9/talos-guides/install/virtualized-platforms/proxmox/}
\begin{quote}
\begin{quote}

\sphinxAtStartPar
talosctl gen config my\sphinxhyphen{}cluster \sphinxurl{https://192.168.0.218:6443}
talosctl \sphinxhyphen{}n 192.168.0.169 get disks \textendash{}insecure (check disks)
talosctl config endpoint 192.168.0.218
talosctl config node 192.168.0.218

\sphinxAtStartPar
talosctl apply\sphinxhyphen{}config \textendash{}insecure \textendash{}nodes 192.168.0.218 \textendash{}file controlplane.yaml

\sphinxAtStartPar
talosctl bootstrap
talosctl kubeconfig . (retrieve kubeconfig)
talosctl \textendash{}nodes 192.168.0.218 version (verify)

\sphinxAtStartPar
export KUBECONFIG=./talos\sphinxhyphen{}config/kubeconfig
\begin{quote}

\sphinxAtStartPar
kubectl get nodes
kubectl get pods \sphinxhyphen{}n kube\sphinxhyphen{}system
kubectl get pods \sphinxhyphen{}n kube\sphinxhyphen{}system \sphinxhyphen{}o wide
\end{quote}
\end{quote}

\sphinxAtStartPar
kubectl describe pod my\sphinxhyphen{}postgres\sphinxhyphen{}postgresql\sphinxhyphen{}0 (is very useful in case the pod does get deployed

\sphinxAtStartPar
\sphinxurl{https://factory.talos.dev/} (create your custom image)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
talosctl\PYG{+w}{ }upgrade\PYG{+w}{ }\PYGZhy{}\PYGZhy{}nodes\PYG{+w}{ }\PYG{l+m}{10}.10.10.178\PYG{+w}{ }\PYGZhy{}\PYGZhy{}image\PYG{+w}{  }factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.9.5
\end{sphinxVerbatim}
\end{quote}
\begin{description}
\sphinxlineitem{watching nodes: {[}10.10.10.178{]}}
\sphinxAtStartPar
talosctl get extensions \textendash{}nodes 10.10.10.178

\end{description}

\sphinxAtStartPar
NODE           NAMESPACE   TYPE              ID   VERSION   NAME          VERSION
10.10.10.178   runtime     ExtensionStatus   0    1         iscsi\sphinxhyphen{}tools   v0.1.6
10.10.10.178   runtime     ExtensionStatus   1    1         schematic     c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac


\section{adding worker nodes}
\label{\detokenize{talos:adding-worker-nodes}}
\sphinxAtStartPar
Since “longhorn” stores data on more than one node, we need to add more nodes to the cluster.
\begin{quote}

\sphinxAtStartPar
talosctl apply\sphinxhyphen{}config \textendash{}insecure \textendash{}nodes 10.10.10.166 \textendash{}file worker.yaml
talosctl apply\sphinxhyphen{}config \textendash{}insecure \textendash{}nodes 10.10.10.173 \textendash{}file worker.yaml
\end{quote}

\sphinxAtStartPar
kubectl get nodes \sphinxhyphen{}o wide
NAME            STATUS   ROLES           AGE     VERSION   INTERNAL\sphinxhyphen{}IP    EXTERNAL\sphinxhyphen{}IP   OS\sphinxhyphen{}IMAGE         KERNEL\sphinxhyphen{}VERSION   CONTAINER\sphinxhyphen{}RUNTIME
talos\sphinxhyphen{}2ho\sphinxhyphen{}roe   Ready    \textless{}none\textgreater{}          113s    v1.32.3   10.10.10.173   \textless{}none\textgreater{}        Talos (v1.9.5)   6.12.18\sphinxhyphen{}talos    containerd://2.0.3
talos\sphinxhyphen{}swn\sphinxhyphen{}isw   Ready    control\sphinxhyphen{}plane   31d     v1.32.3   10.10.10.118   \textless{}none\textgreater{}        Talos (v1.9.5)   6.12.18\sphinxhyphen{}talos    containerd://2.0.3
talos\sphinxhyphen{}v1x\sphinxhyphen{}9s4   Ready    \textless{}none\textgreater{}          2m18s   v1.32.3   10.10.10.166   \textless{}none\textgreater{}        Talos (v1.9.5)   6.12.18\sphinxhyphen{}talos    containerd://2.0.3
talos\sphinxhyphen{}y7t\sphinxhyphen{}8ll   Ready    worker          29d     v1.32.3   10.10.10.178   \textless{}none\textgreater{}        Talos (v1.9.5)   6.12.18\sphinxhyphen{}talos    containerd://2.0.3


\section{label nodes}
\label{\detokenize{talos:label-nodes}}\begin{quote}

\sphinxAtStartPar
kubectl label nodes talos\sphinxhyphen{}v1x\sphinxhyphen{}9s4 node\sphinxhyphen{}role.kubernetes.io/worker=””
kubectl label nodes talos\sphinxhyphen{}2ho\sphinxhyphen{}roe node\sphinxhyphen{}role.kubernetes.io/worker=””
\end{quote}

\sphinxstepscope


\chapter{Kernel IP Routing Table}
\label{\detokenize{routing-table:kernel-ip-routing-table}}\label{\detokenize{routing-table::doc}}
\sphinxAtStartPar
The following table represents the kernel IP routing table:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Kernel IP Routing Table}\label{\detokenize{routing-table:id1}}
\sphinxaftertopcaption
\begin{tabular}[t]{\X{15}{75}\X{15}{75}\X{15}{75}\X{5}{75}\X{5}{75}\X{5}{75}\X{5}{75}\X{10}{75}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Destination
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Gateway
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Genmask
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Flags
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Ref
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Use
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Iface
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
default
&
\sphinxAtStartPar
192.168.0.1
&
\sphinxAtStartPar
0.0.0.0
&
\sphinxAtStartPar
UG
&
\sphinxAtStartPar
100
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxhline
\sphinxAtStartPar
default
&
\sphinxAtStartPar
192.168.0.1
&
\sphinxAtStartPar
0.0.0.0
&
\sphinxAtStartPar
UG
&
\sphinxAtStartPar
1024
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxhline
\sphinxAtStartPar
10.10.10.0
&
\sphinxAtStartPar
0.0.0.0
&
\sphinxAtStartPar
255.255.255.0
&
\sphinxAtStartPar
U
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth0
\\
\sphinxhline
\sphinxAtStartPar
192.168.0.0
&
\sphinxAtStartPar
0.0.0.0
&
\sphinxAtStartPar
255.255.255.0
&
\sphinxAtStartPar
U
&
\sphinxAtStartPar
1024
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxhline
\sphinxAtStartPar
192.168.0.1
&
\sphinxAtStartPar
0.0.0.0
&
\sphinxAtStartPar
255.255.255.255
&
\sphinxAtStartPar
UH
&
\sphinxAtStartPar
1024
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxhline
\sphinxAtStartPar
gent.dnscache01
&
\sphinxAtStartPar
192.168.0.1
&
\sphinxAtStartPar
255.255.255.255
&
\sphinxAtStartPar
UGH
&
\sphinxAtStartPar
1024
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxhline
\sphinxAtStartPar
gent.dnscache02
&
\sphinxAtStartPar
192.168.0.1
&
\sphinxAtStartPar
255.255.255.255
&
\sphinxAtStartPar
UGH
&
\sphinxAtStartPar
1024
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
eth1
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{SDA (software defined architecture)}
\label{\detokenize{SDA:sda-software-defined-architecture}}\label{\detokenize{SDA::doc}}

\section{Physical Infrastructure}
\label{\detokenize{SDA:physical-infrastructure}}

\section{Proxmox Server Specifications}
\label{\detokenize{SDA:proxmox-server-specifications}}
\sphinxAtStartPar
The foundation of the architecture is a physical server running Proxmox VE hypervisor.


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{70}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Component
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Hypervisor}
&
\sphinxAtStartPar
Proxmox Virtual Environment (VE)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Virtual Machines}
&
\sphinxAtStartPar
Multiple Talos OS nodes forming a Kubernetes cluster
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Containers}
&
\sphinxAtStartPar
LXC container serving as a router
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{Virtual Machine Configuration}
\label{\detokenize{SDA:virtual-machine-configuration}}
\begin{sphinxadmonition}{note}{Talos OS Nodes}

\sphinxAtStartPar
The cluster consists of multiple Talos OS nodes, with dedicated roles:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Control Plane Node(s)}: Manages the Kubernetes control plane

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Worker Nodes}: Runs application workloads

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZsh{} }Example\PYG{+w}{ }Talos\PYG{+w}{ }configuration\PYG{+w}{ }structure\PYG{+w}{ }\PYG{o}{(}simplified\PYG{o}{)}
\PYG{g+go}{machine:}
\PYG{g+go}{  type: controlplane  \PYGZsh{} or worker}
\PYG{g+go}{  network:}
\PYG{g+go}{    hostname: talos\PYGZhy{}node\PYGZhy{}1}
\PYG{g+go}{  kubernetes:}
\PYG{g+go}{    version: v1.26.0}
\end{sphinxVerbatim}


\section{Networking Architecture}
\label{\detokenize{SDA:networking-architecture}}

\subsection{Network Components}
\label{\detokenize{SDA:network-components}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{70}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Component
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Function
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Proxmox Virtual Bridge}
&
\sphinxAtStartPar
Creates isolated network segments for VMs and containers
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{LXC Router}
&
\sphinxAtStartPar
Routes traffic between internal and external networks
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Kubernetes Overlay Network}
&
\sphinxAtStartPar
Enables pod\sphinxhyphen{}to\sphinxhyphen{}pod communication (Cilium, Flannel, etc.)
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{Control \& Automation}
\label{\detokenize{SDA:control-automation}}

\subsection{API Management Layer}
\label{\detokenize{SDA:api-management-layer}}
\sphinxAtStartPar
This architecture leverages multiple declarative APIs for infrastructure management:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{25}{100}\X{75}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
API
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Responsibility
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Proxmox API}
&
\sphinxAtStartPar
Manages physical resources, VMs, and containers
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Talos API}
&
\sphinxAtStartPar
Provides declarative OS configuration and maintenance
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Kubernetes API}
&
\sphinxAtStartPar
Orchestrates applications and services
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{Benefits of This Architecture}
\label{\detokenize{SDA:benefits-of-this-architecture}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Immutable Infrastructure}: Talos OS provides an immutable, declarative operating system

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High Availability}: Kubernetes manages service availability and distribution

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Efficiency}: Consolidates multiple services on a single physical server

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Isolation}: Separate network segments and container boundaries

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automation}: API\sphinxhyphen{}driven management at all levels

\end{itemize}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}